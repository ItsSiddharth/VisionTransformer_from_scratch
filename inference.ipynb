{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/anaconda3/envs/dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = load_dataset(\"cifar10\")\n",
    "\n",
    "# Define a transformation to convert PIL images to tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261]),\n",
    "])\n",
    "\n",
    "# Select subset of data\n",
    "train_data = cifar10[\"train\"].select(range(200))\n",
    "test_data = cifar10[\"test\"].select(range(100))\n",
    "\n",
    "# Create PyTorch Dataset class\n",
    "class Cifar10Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample[\"img\"]\n",
    "        label = sample[\"label\"]\n",
    "        if self.transform:\n",
    "            print(image.size)\n",
    "            image = self.transform(image.convert(\"RGB\")) # Convert PIL image to Tensor\n",
    "\n",
    "        return image, label\n",
    "\n",
    "train_dataset = Cifar10Dataset(train_data, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(32, 32)\n",
      "torch.Size([32, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/anaconda3/envs/dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import ViTforImageClassification\n",
    "from dataset import get_cifar10_dataloader\n",
    "\n",
    "from config_json import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "# Define training parameters\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # This is for apple silicon\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "learning_rate = config[\"lr\"]\n",
    "save_path = \"best_vit_model.pth\"\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ViTforImageClassification(config).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader, val_data_loader = get_cifar10_dataloader(batch_size=config[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.8790e+00, -5.5612e-01, -2.1052e+00,  4.0457e+00,  8.5984e-01,\n",
      "          7.7154e+00, -6.9422e-01, -5.2390e+00, -3.9817e+00, -1.9394e-01],\n",
      "        [-1.3675e+00, -1.9447e-01, -4.8972e-02, -2.0157e+00,  1.3705e-02,\n",
      "         -9.7101e-01, -4.3103e+00, -2.8027e+00,  1.4293e+00,  7.5641e+00],\n",
      "        [ 2.7155e+00, -2.4774e+00,  2.0959e+00,  6.2772e-01,  2.0166e+00,\n",
      "         -4.9911e+00,  1.6773e+00,  6.9790e+00, -2.2916e+00, -2.5475e+00],\n",
      "        [ 2.6525e+00, -4.8836e-01, -1.5520e+00, -1.6419e+00, -8.4108e-01,\n",
      "         -2.5846e+00, -5.0165e+00, -7.1271e-01,  8.6391e+00,  1.7626e+00],\n",
      "        [ 4.9850e-01,  5.0055e+00, -1.7327e+00,  2.1353e+00, -2.7851e+00,\n",
      "         -1.1528e+00,  1.7800e-01, -4.4024e+00,  1.1931e+00,  1.7681e-01],\n",
      "        [-1.6306e+00,  7.6581e-01,  3.3112e+00, -1.0177e-02, -2.6529e+00,\n",
      "          6.0321e+00,  2.2091e+00, -3.2034e+00, -1.4141e+00, -4.2555e+00],\n",
      "        [-1.0632e+00,  5.9228e+00,  1.3692e+00, -2.6003e-01, -4.9659e+00,\n",
      "         -1.5858e+00, -5.4314e-01, -3.5114e+00,  2.4482e+00,  8.3294e-01],\n",
      "        [ 1.5531e+00,  5.3482e+00, -3.7395e+00,  1.6328e+00, -2.0034e+00,\n",
      "         -3.8060e+00, -2.1791e+00,  3.1770e+00, -8.8097e-01,  1.2019e+00],\n",
      "        [ 2.8264e+00,  3.4962e+00, -3.2012e+00, -1.0085e+00,  1.5934e-01,\n",
      "         -2.0357e+00, -4.2609e+00,  2.6599e+00,  4.2527e-01,  1.9715e+00],\n",
      "        [ 3.1225e+00,  5.5887e-01, -1.1682e+00,  9.8198e-01, -4.1738e+00,\n",
      "         -1.6073e+00,  5.7062e+00,  2.2030e+00,  2.0584e-01, -2.8071e+00],\n",
      "        [ 3.6301e+00,  1.8643e+00,  2.5982e+00, -4.5772e-01, -4.9266e+00,\n",
      "         -4.3050e+00, -3.4944e+00, -1.3143e+00,  3.4790e+00,  2.3665e+00],\n",
      "        [-3.0898e-01, -6.2799e-01, -3.8315e+00,  1.4297e+00,  1.9469e+00,\n",
      "          6.1001e+00, -2.9569e+00, -1.2853e+00, -4.3797e+00,  8.1155e-01],\n",
      "        [-6.4021e-03,  3.1640e+00,  5.4334e-01,  2.6690e+00, -2.6327e+00,\n",
      "         -4.5431e+00,  6.8615e+00,  3.3853e-02, -2.9009e+00, -2.5035e+00],\n",
      "        [ 1.7037e+00,  2.3179e+00, -3.5469e+00, -2.9155e-01, -2.2413e-01,\n",
      "          3.1611e+00, -1.3056e+00, -3.4543e+00, -4.5372e+00,  4.6327e+00],\n",
      "        [ 6.3527e-01, -4.2463e+00,  1.3846e+00, -6.8137e-01,  7.8879e+00,\n",
      "          1.1463e+00, -2.2418e-01,  2.1756e-01, -3.6215e+00, -1.6087e+00],\n",
      "        [ 2.6138e-02, -6.4721e-01,  9.1519e-01, -2.7604e+00, -2.5507e+00,\n",
      "          4.4705e-01, -3.0000e+00, -5.4900e+00,  6.4829e+00,  3.1595e+00],\n",
      "        [-1.2926e+00, -2.9663e+00, -8.8656e-01,  2.6220e+00,  6.6605e+00,\n",
      "          2.7865e+00, -5.6869e-01,  2.3539e+00, -3.3181e+00, -4.0810e+00],\n",
      "        [ 2.6761e-01, -2.3151e+00, -2.0336e+00,  1.5912e+00,  9.1846e-01,\n",
      "          8.9063e+00, -1.0173e-02, -4.7564e+00, -3.9689e+00, -8.5969e-01],\n",
      "        [ 2.8717e+00, -1.0113e+00, -2.3852e+00,  1.7616e+00,  3.1588e+00,\n",
      "         -2.6158e+00,  1.5732e+00,  7.2276e+00, -1.7730e+00, -5.8926e+00],\n",
      "        [ 1.6898e+00,  2.1475e+00,  4.6747e-01, -1.6162e+00, -2.3207e+00,\n",
      "         -4.0550e+00, -3.7047e+00, -2.7445e+00,  4.9019e+00,  4.6542e+00],\n",
      "        [ 1.0252e+00, -2.5917e+00, -4.2059e-01, -9.7076e-01,  3.8509e+00,\n",
      "          7.2285e+00, -1.2810e+00, -1.8303e+00, -1.6932e+00, -3.5046e+00],\n",
      "        [-3.6047e+00,  6.1850e-01,  2.9185e-01, -2.7936e-01, -1.2151e+00,\n",
      "          6.9740e-01,  2.3749e-01, -5.9214e+00, -3.3319e+00,  8.0500e+00],\n",
      "        [-4.0069e-01, -3.2068e+00,  6.3028e+00,  1.1681e-01,  1.2276e+00,\n",
      "          8.2469e-01,  3.5814e+00,  2.3282e-01, -3.7470e+00, -4.2252e+00],\n",
      "        [ 5.6452e+00, -1.3858e+00,  2.5528e+00, -2.2862e+00, -2.7880e+00,\n",
      "          1.9718e+00, -1.1454e-01, -2.5119e-02,  8.1629e-01, -3.2285e+00],\n",
      "        [ 7.6655e+00,  2.5621e-01, -1.0629e+00,  6.7887e-02, -3.0735e+00,\n",
      "         -2.8976e+00, -9.4118e-01,  2.6339e+00,  3.5362e+00, -4.2095e+00],\n",
      "        [ 2.5447e+00, -4.4248e+00, -6.0031e-01, -1.0195e+00,  3.0622e+00,\n",
      "          6.8937e+00, -3.2778e+00, -1.9200e+00, -3.0442e+00, -2.1514e-01],\n",
      "        [ 3.6763e+00,  4.3923e+00, -3.1841e+00, -1.5468e+00,  3.5250e-01,\n",
      "         -7.4254e-01, -3.4681e+00, -8.7328e-01, -1.4186e+00,  3.3647e+00],\n",
      "        [ 7.9595e+00,  8.3873e-01, -1.8820e+00, -1.7255e+00, -1.8715e+00,\n",
      "          1.0960e-01, -3.5982e+00,  2.5123e+00, -9.0611e-01, -1.6768e-01],\n",
      "        [-1.9622e+00, -2.9377e+00,  1.4709e+00,  3.0331e+00,  6.0095e+00,\n",
      "          1.5897e-01,  1.0422e+00,  1.9557e+00, -5.9335e+00, -2.8075e+00],\n",
      "        [-4.4730e+00, -1.0758e+00,  6.6499e+00, -3.6223e-01,  1.6712e+00,\n",
      "          1.7570e+00, -3.0911e+00, -1.2737e+00, -3.1948e+00,  7.1830e-01],\n",
      "        [-1.1991e+00, -3.4756e+00,  5.8176e-01,  5.9888e-01,  7.0248e+00,\n",
      "         -6.0412e-02, -1.0814e+00,  4.4129e+00, -3.8010e+00, -2.0573e+00],\n",
      "        [ 3.6130e+00, -3.6696e+00, -8.7824e-01, -2.4854e+00,  6.5981e+00,\n",
      "          7.1460e-01, -2.0588e+00,  1.0346e+00, -2.5312e+00,  1.6446e-01]],\n",
      "       device='mps:0', grad_fn=<LinearBackward0>)\n",
      "torch.Size([32, 10])\n",
      "tensor([5, 9, 7, 8, 1, 5, 1, 1, 1, 6, 0, 5, 6, 9, 4, 8, 4, 5, 7, 8, 5, 9, 2, 0,\n",
      "        0, 5, 1, 0, 4, 2, 4, 4], device='mps:0') torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    loop = tqdm(train_data_loader, leave=True)\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, outputs = model(images, labels)\n",
    "        print(outputs)\n",
    "        print(outputs.shape)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        print(predicted, predicted.shape)\n",
    "        break\n",
    "    break\n",
    "        # total += labels.size(0)\n",
    "        # correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
